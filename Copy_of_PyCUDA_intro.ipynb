{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of PyCUDA_intro.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marimiya/hello-world/blob/master/Copy_of_PyCUDA_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31d0lBm-Lm6o",
        "colab_type": "text"
      },
      "source": [
        "**Part 1**\n",
        "First of all you need to check whether you have a GPU-accelerated Runtime. To do this, Select the 'Runtime' menu and then choose 'Change runtime type'. You should then choose 'GPU' if it is not already selected. Once done, you may need to select the 'Connect' option at the top right of your notebook window. \n",
        "When it has loaded and connected you should run the following command to check whether the system is indeed using a GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjnf4EoLLQqL",
        "colab_type": "code",
        "outputId": "76b95b5d-d8cd-45a3-c725-ad313fdfcec3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Oct 24 02:14:37 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.50       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24l8u6jxM0rH",
        "colab_type": "text"
      },
      "source": [
        "If all is well, you should see some output that includes the type of GPU being used (k80) and lists any processed running (none for the moment).\n",
        "\n",
        "Next, we need to install PyCUDA:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOSSKBYcPK2y",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0VbZq97ONuD",
        "colab_type": "code",
        "outputId": "16346160-9fa7-45da-9151-1cf5a165ccfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "pip install pycuda"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pycuda\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/3f/5658c38579b41866ba21ee1b5020b8225cec86fe717e4b1c5c972de0a33c/pycuda-2019.1.2.tar.gz (1.6MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 3.5MB/s \n",
            "\u001b[?25hCollecting pytools>=2011.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/96/00416762a3eda8876a17d007df4a946f46b2e4ee1057e0b9714926472ef8/pytools-2019.1.1.tar.gz (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from pycuda) (4.4.0)\n",
            "Collecting appdirs>=1.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/56/eb/810e700ed1349edde4cbdc1b2a21e28cdf115f9faf263f6bbf8447c1abf3/appdirs-1.4.3-py2.py3-none-any.whl\n",
            "Collecting mako\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/3c/8dcd6883d009f7cae0f3157fb53e9afb05a0d3d33b3db1268ec2e6f4a56b/Mako-1.1.0.tar.gz (463kB)\n",
            "\u001b[K     |████████████████████████████████| 471kB 43.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.16.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako->pycuda) (1.1.1)\n",
            "Building wheels for collected packages: pycuda, pytools, mako\n",
            "  Building wheel for pycuda (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2019.1.2-cp36-cp36m-linux_x86_64.whl size=4536167 sha256=d9bad67d3a97ed3dacf8f04ca196d3daf26b6877eeea0a1f47e3ed46c2141588\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/60/f0/b1c430c73d281ac3e46070480db50f7907364eb6f6d3188396\n",
            "  Building wheel for pytools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytools: filename=pytools-2019.1.1-py2.py3-none-any.whl size=58424 sha256=37bba396e1ba6f5c98623e764a12618d4e678e02aa02258d0487996505142b84\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/df/0b/75ac4572aaa93e3eba6a58472635d0fda907f5f4cf884a3a0c\n",
            "  Building wheel for mako (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mako: filename=Mako-1.1.0-cp36-none-any.whl size=75363 sha256=2d96c964db01ede64e6dfb827f4c29628364e6c965488927bf1cf065b27f554c\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/32/7b/a291926643fc1d1e02593e0d9e247c5a866a366b8343b7aa27\n",
            "Successfully built pycuda pytools mako\n",
            "Installing collected packages: appdirs, pytools, mako, pycuda\n",
            "Successfully installed appdirs-1.4.3 mako-1.1.0 pycuda-2019.1.2 pytools-2019.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r3S8FLxPNz0",
        "colab_type": "text"
      },
      "source": [
        "Now we should be able to import the PyCUDA libraries that we need:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUxj_OKmNQg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from pycuda.compiler import SourceModule"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l11ln93PYwX",
        "colab_type": "text"
      },
      "source": [
        "We will be using numpy quite a lot so let's import that right away:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNoxx_f-PzhF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVY_apz0QTb5",
        "colab_type": "text"
      },
      "source": [
        "Okay, let's create a 4 x 4 array using numpy's random function and show the contents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6NdnDdoP8K5",
        "colab_type": "code",
        "outputId": "31e3c8aa-adca-48b6-e33d-5af1d93e663a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "a = np.random.randn(4,4)\n",
        "a"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.64958587,  0.24983324, -0.89895253, -0.801612  ],\n",
              "       [ 0.30447968, -2.07722494,  1.35775999, -2.04195412],\n",
              "       [-0.65881711,  0.88978129, -0.2129629 ,  2.63858029],\n",
              "       [ 0.70924416,  0.45147205,  0.60061799, -0.02043707]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGTfLWQ7Q3Ns",
        "colab_type": "text"
      },
      "source": [
        "Let's check the type of one of the elements:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsXrbyKiQnhv",
        "colab_type": "code",
        "outputId": "278c99f5-e0d5-4cf8-d1a6-8df4e4cefd3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(a[0,0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUESvwNERCT-",
        "colab_type": "text"
      },
      "source": [
        "As you should see, this is of type float64. This is not always the right data type for kernel functions for the following reasons:\n",
        "Only Enterprise GPUs (i.e. Volta/Pascal rather than GeForce/RTX) tend to support Full Precision (float64). Even then, they tend to have better support for lower precision (e.g. int32, fp16 etc.)\n",
        "So it's better to either specify the type or cast it before passing it to the GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTys3x--SiBk",
        "colab_type": "code",
        "outputId": "f74eb0a6-21c1-42a8-b186-73fc8284ac96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "b = np.random.randn(4,4)\n",
        "b = b.astype(np.float32)\n",
        "print(b)\n",
        "print(type(b[0,0]))\n",
        "print(\"\")\n",
        "c = np.random.randint(low=1, high=100, size=16, dtype=np.int16).reshape(4,4)\n",
        "print(c)\n",
        "print(type(c[0,0]))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-1.9560276  -0.19318388 -1.3799247  -0.23417418]\n",
            " [ 0.449587    0.8313106   2.4740198  -1.293046  ]\n",
            " [ 0.44903103  1.6990381  -0.845149   -0.5263452 ]\n",
            " [-0.6007457   1.3219384  -0.6088996  -0.35990486]]\n",
            "<class 'numpy.float32'>\n",
            "\n",
            "[[34 62 19 84]\n",
            " [48 97 59 60]\n",
            " [25 29 66 17]\n",
            " [53 45  2 17]]\n",
            "<class 'numpy.int16'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh6tEEn7Xoxx",
        "colab_type": "text"
      },
      "source": [
        "Okay, now let's take a look at the way our data can be transferred over to the device (GPU) for a kernel to act upon.\n",
        "In this example, we use the cuda.mem_alloc to allocate some gpu memory. In order to do this, we need to determine how much memory we need. For this we use the nbytes property of our variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grrpCarZSTIc",
        "colab_type": "code",
        "outputId": "44074a17-c3c5-4b78-8168-947396fafd80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "b_gpu = cuda.mem_alloc(b.nbytes)\n",
        "print(str(b.nbytes) + \" bytes allocated\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "64 bytes allocated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftKgGFYqZNRQ",
        "colab_type": "text"
      },
      "source": [
        "That has allocated the memory, so now we can transfer the data to the gpu memory (htod = host to device):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilnj0yVIZXjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuda.memcpy_htod(b_gpu, b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfkhGj32aCal",
        "colab_type": "text"
      },
      "source": [
        "Next we need to create some code to execute on the GPU - our kernel function - that will operate on the data we just passed.\n",
        "\n",
        "In this case all we are going to do is multiply the contents of the a matrix by 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wLUQDP2abuh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mod = SourceModule(\"\"\"\n",
        "  __global__ void doublify(float *a)\n",
        "  {\n",
        "    int idx = threadIdx.x + threadIdx.y*4;\n",
        "    a[idx] *= 2;\n",
        "  }\n",
        "  \"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp3lU5sNa5mb",
        "colab_type": "text"
      },
      "source": [
        "If there were no errors then our kernel has compiled and is ready to be called."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmaDMGh7bDKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "func = mod.get_function(\"doublify\")\n",
        "func(b_gpu, block=(4,4,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_sbUERnbZjC",
        "colab_type": "text"
      },
      "source": [
        "Let's just take a look at what we just executed.\n",
        "First we got a handle to the doublify function that we just created, then we called the function passing a_gpu as the function argument and we specified the launch configuration using the block parameter - more on this shortly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYwqtV2lcIj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b_doubled = np.empty_like(b)\n",
        "cuda.memcpy_dtoh(b_doubled, b_gpu)\n",
        "print(b_doubled)\n",
        "print(b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbcEfBK2e88m",
        "colab_type": "text"
      },
      "source": [
        "Okay, so that worked (hopefully) but what did the kernel code actually do? \n",
        "To more fully understand this we need to take a look at the way that kernels are launched (Slides will be presented).\n",
        "\n",
        "**Part 2**\n",
        "\n",
        "To better understand what is going on when you launch a kernel let's try a few experiments.\n",
        "\n",
        "We'll create a simple kernel that just sets the elements of the input array to the thread index. This should help show what is going on\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsC5AkKgBuWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mod2 = SourceModule(\"\"\"\n",
        "  __global__ void getInfo(int *a)\n",
        "  {\n",
        "    int idx = threadIdx.x;\n",
        "    a[idx] = threadIdx.x;\n",
        "\n",
        "  }\n",
        "  \"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-drv0TiZuMx",
        "colab_type": "code",
        "outputId": "8a1eb7d2-fca8-45f7-e57a-1da09f0f9207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Create an empty 1d array\n",
        "d = np.zeros((16), dtype=np.uint32)\n",
        "# Allocate GPU memory\n",
        "d_gpu = cuda.mem_alloc(d.nbytes)\n",
        "# clear the gpu memory\n",
        "cuda.memcpy_htod(d_gpu, d)\n",
        "\n",
        "# Get the function handle\n",
        "func2 = mod2.get_function(\"getInfo\")\n",
        "# Create a lauch configuration with 4 threads and 1 block\n",
        "func2(d_gpu, block=(4,1,1))\n",
        "\n",
        "# Create array for output\n",
        "d_out = np.empty_like(d)\n",
        "# Copy the gpu memory to the host\n",
        "cuda.memcpy_dtoh(d_out, d_gpu)\n",
        "\n",
        "# Print data out\n",
        "print(d_out)\n",
        "\n",
        "# Free up the memory\n",
        "d_gpu.free()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erlB6WMUre3l",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaIq0qV6cR2C",
        "colab_type": "text"
      },
      "source": [
        "Do you understand what the output has produced? \n",
        "Our kernel has launched a block with 4 threads (block = (4,1,1) means that 4 threads are created in the block on the x dimension, with no additional threads on the y or z dimensions). Each thread has written its threadIdx.x value into the array element corresponding to this value. Since there are only 4 threads, only the first 4 elements have been updated.\n",
        "If we want to write into the other elements we will have to do one of several things:\n",
        "\n",
        "*   Increase the number of threads in the block\n",
        "*   Increase the number of blocks\n",
        "*   Iterate within the kernel\n",
        "\n",
        "Let's try these one by one.\n",
        "\n",
        "By changing the number of threads per block to 16 (in the x dimension) we can fully populate the array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1sGcy_3dzMe",
        "colab_type": "code",
        "outputId": "550dd55a-264e-4a45-c5b3-70d9b76235bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Allocate GPU memory\n",
        "d_gpu = cuda.mem_alloc(d.nbytes)\n",
        "# clear the gpu memory\n",
        "cuda.memcpy_htod(d_gpu, d)\n",
        "\n",
        "# Create a lauch configuration with 416 threads\n",
        "func2(d_gpu, block=(16,1,1))\n",
        "\n",
        "# Copy the gpu memory to the host\n",
        "cuda.memcpy_dtoh(d_out, d_gpu)\n",
        "\n",
        "# Present data as a matrix\n",
        "print(d_out)\n",
        "\n",
        "# Free gpu memory\n",
        "d_gpu.free()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN7jDBz7Nkt4",
        "colab_type": "text"
      },
      "source": [
        "In order to make increasing the number of blocks effective we will need to change the kernel so that it can use them. Without doing this each block will simply write into the same array element as the others. By adding the blockIdx.x into the kernel we can use it to map each thread and block to a specific element."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S07MPK31Omp5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mod3 = SourceModule(\"\"\"\n",
        "  __global__ void getInfo(int *a)\n",
        "  {\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    a[idx] = idx;\n",
        "\n",
        "  }\n",
        "  \"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNR9XHJyMwuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Allocate GPU memory\n",
        "d_gpu = cuda.mem_alloc(d.nbytes)\n",
        "# clear the gpu memory\n",
        "cuda.memcpy_htod(d_gpu, d)\n",
        "\n",
        "# Get the function handle\n",
        "func3 = mod3.get_function(\"getInfo\")\n",
        "# Create a lauch configuration with 4 threads and 4 blocks\n",
        "func3(d_gpu, block=(4,1,1), grid=(4,1,1))\n",
        "\n",
        "# Copy the gpu memory to the host\n",
        "cuda.memcpy_dtoh(d_out, d_gpu)\n",
        "\n",
        "# Present data as a matrix\n",
        "print(d_out)\n",
        "\n",
        "# Free gpu memory\n",
        "d_gpu.free()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_Got-CQRVvm",
        "colab_type": "text"
      },
      "source": [
        "Lets try some iteration within the kernel next. This is only really used when the size of the problem exceeds the total number of threads available on the gpu but, for the sake of illustration we'll try it here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtJ7QYbNR5fO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mod4 = SourceModule(\"\"\"\n",
        "  __global__ void getInfo(int *a)\n",
        "  {\n",
        "    int idx = threadIdx.x;\n",
        "    for (int i=0;i<16;i+=4)\n",
        "    {\n",
        "      a[idx + i] = idx + i;\n",
        "    }\n",
        "  }\n",
        "  \"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Dpd7M-1Tesh",
        "colab_type": "code",
        "outputId": "4ece6f7b-96c7-4c77-a51f-5f91d491addf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Allocate GPU memory\n",
        "d_gpu = cuda.mem_alloc(d.nbytes)\n",
        "# clear the gpu memory\n",
        "cuda.memcpy_htod(d_gpu, d)\n",
        "\n",
        "# Get the function handle\n",
        "func3 = mod4.get_function(\"getInfo\")\n",
        "# Create a lauch configuration with 4 threads and 4 blocks\n",
        "func3(d_gpu, block=(4,1,1))\n",
        "\n",
        "# Copy the gpu memory to the host\n",
        "cuda.memcpy_dtoh(d_out, d_gpu)\n",
        "\n",
        "# Present data as a matrix\n",
        "print(d_out)\n",
        "\n",
        "# Free gpu memory\n",
        "d_gpu.free()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3qpuTdnUP42",
        "colab_type": "text"
      },
      "source": [
        "Okay, so as a final exercise, let's see if the following makes sense. Note that we are using a 3D matrix as input this time. However, as far as the C-based kernel is concerned, there is no difference between a 3D array and a 1D array - ultimately they are both just contiguous memory.\n",
        "Check that you understand the output. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rQ4aw8gJZu7W",
        "colab": {}
      },
      "source": [
        "mod5 = SourceModule(\"\"\"\n",
        "  __global__ void getInfo(int *a)\n",
        "  {\n",
        "    int idx = 4 * (blockIdx.x * blockDim.x + threadIdx.x);\n",
        "    a[idx] = threadIdx.x;\n",
        "    a[idx + 1] = blockIdx.x;\n",
        "    a[idx + 2] = blockDim.x;\n",
        "    a[idx + 3] = gridDim.x;\n",
        "  }\n",
        "  \"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwr8Ok8WGheO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an empty 3d array\n",
        "b = np.zeros((4,4,4), dtype=np.uint32)\n",
        "# Allocate GPU memory\n",
        "b_gpu = cuda.mem_alloc(b.nbytes)\n",
        "\n",
        "# Get the function handle\n",
        "func = mod5.get_function(\"getInfo\")\n",
        "# Create a lauch configuration with 4 threads and 4 blocks\n",
        "func(b_gpu, block=(4,1,1), grid=(4,1,1))\n",
        "\n",
        "# Create array for output\n",
        "b_out = np.empty_like(b)\n",
        "# Copy the gpu memory to the host\n",
        "cuda.memcpy_dtoh(b_out, b_gpu)\n",
        "\n",
        "# Present data as a matrix\n",
        "print(b_out.reshape((4,4,4)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qKl-fvuU2P1",
        "colab_type": "text"
      },
      "source": [
        "Try changing some of the parameters such as block size and see what happens\n",
        "\n",
        "Finally, see if you can use what you have learned to create kernel that multiplies the elements in matrix a by matrix b. The outline has been provided. Please see if you can complete the code and produce a correct result!\n",
        "The assertion will fail if you don't. Remember to edit the launch configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uITt2WiVhEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mmult = SourceModule(\"\"\"\n",
        "  __global__ void multiply(int *a, int *b)\n",
        "  {\n",
        "    int idx = TODO;\n",
        "    // multiply each element of a by b \n",
        "    // TODO\n",
        "  }\n",
        "  \"\"\")\n",
        "\n",
        "a = np.array([[1,2,3,4],[1,2,3,4],[1,2,3,4],[1,2,3,4]],dtype=np.uint32)\n",
        "b = np.array([[1,1,1,1],[2,2,2,2],[3,3,3,3],[4,4,4,4]],dtype=np.uint32)\n",
        "\n",
        "# Allocate GPU memory\n",
        "a_gpu = cuda.mem_alloc(a.nbytes)\n",
        "b_gpu = cuda.mem_alloc(b.nbytes)\n",
        "# set the gpu memory\n",
        "cuda.memcpy_htod(a_gpu, a)\n",
        "cuda.memcpy_htod(b_gpu, b)\n",
        "\n",
        "# Get the function handle\n",
        "func = mmult.get_function(\"multiply\")\n",
        "# Create a lauch configuration with 4 threads and 1 block\n",
        "func(a_gpu, b_gpu, TODO ......)\n",
        "\n",
        "# Create array for output\n",
        "a_out = np.empty_like(a)\n",
        "# Copy the gpu memory to the host\n",
        "cuda.memcpy_dtoh(a_out, a_gpu)\n",
        "\n",
        "# Print data out\n",
        "print(a_out)\n",
        "\n",
        "# Free up the memory\n",
        "a_gpu.free()\n",
        "b_gpu.free()\n",
        "\n",
        "assert (a_out == [[1,2,3,4],[2,4,6,8],[3,6,9,12],[4,8,12,16]]).all()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMIyO-97y4QP",
        "colab_type": "text"
      },
      "source": [
        "There is a simpler way of handling arrays than what we have so far looked at. The GPUArray is an abstraction that removes some of the steps. It is useful to understand what those steps did though, because it helps your understanding of what is going on, which is helpful when it comes to getting the best performance from your code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUCuy7gFzg9c",
        "colab_type": "code",
        "outputId": "9c28eb8e-9e04-471c-f8cc-bc1cace162b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "from pycuda import gpuarray\n",
        "\n",
        "x = np.random.randn(5).astype(np.float32)\n",
        "x_gpu = gpuarray.to_gpu(x)\n",
        "\n",
        "x_gpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.99748343, -0.54913306, -0.15226158,  0.47780636,  0.44367114],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e87omznK02qw",
        "colab_type": "text"
      },
      "source": [
        "You can operate directly on gpuarrays as if they were normal numpy arrays.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzJHO54Z3GAM",
        "colab_type": "code",
        "outputId": "d50f76bd-2f1e-4b9f-b76a-5583650bce18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "x_gpu = x_gpu * 3\n",
        "x_gpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.9924502 , -1.6473992 , -0.45678475,  1.4334191 ,  1.3310134 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1PZq0gp3gNY",
        "colab_type": "text"
      },
      "source": [
        "GPUArrays also support slicing. The one thing that you cannot do is indexing though:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elYXoJ6Y3wrC",
        "colab_type": "code",
        "outputId": "641e0c61-8cdf-4acb-e817-9a654581e6f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_gpu[:3]\n",
        "\n",
        "x_gpu[3]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(1.4334191, dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhdgyxHX4Z_k",
        "colab_type": "text"
      },
      "source": [
        "In order to use a GPUArray in a function created with SourceModule (as per the examples above) we need to use the **gpudata** attribute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjwGPGvl5JXU",
        "colab_type": "code",
        "outputId": "20fae7d6-d4c5-4468-ccaa-a4601ea4c021",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        " func = mod.get_function(\"doublify\")\n",
        " func(x_gpu.gpudata, block = (5,1,1))\n",
        "\n",
        " x_gpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-5.9849005, -3.2947984, -0.9135695,  2.8668382,  2.662027 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMqpyFFM6YOk",
        "colab_type": "text"
      },
      "source": [
        "As you can see, this is much cleaner than the previous method! \n",
        "\n",
        "Next we need to explain a couple of important concepts that, so far, we ignored.\n",
        "First of all, you may have noticed that the kernel function we declared was preceeded by the \\_\\_global\\_\\_ keyword. This tells the compiler that this is kernel (i.e. it will be called from the CPU but executed on the GPU). You will also have seen that the return type was 'void'. This is because kernel function cannot return values. Instead, any results need to be handled by passing in a pointer to a variable and manipulating the data within the kernel.\n",
        "\n",
        "Additionally, sometimes you need to call a function from within your kernel. This can be done by declaring a \\_\\_device\\_\\_ function, which will reside only on the GPU.\n",
        "\n",
        "Let's create a simple example of a device function:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKzcBhRgCa2r",
        "colab_type": "code",
        "outputId": "d839fbeb-b227-4fda-c928-9787e455f8d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dev = SourceModule(\"\"\"\n",
        "__device__ int times2( float px); // declaration\n",
        "  __global__ void  matAdd(float *a)  \n",
        "{  \n",
        "        int idx = blockIdx.x*blockDim.x+threadIdx.x;  \n",
        "        \n",
        "        a[idx] = times2(idx);   \n",
        "\n",
        "}  \n",
        "  __device__ int times2( float idx) // implementation\n",
        "{\n",
        "    return idx * 2; \n",
        "}  \n",
        "\"\"\")\n",
        "\n",
        "func = dev.get_function(\"matAdd\")\n",
        "func(x_gpu.gpudata, block = (5,1,1))\n",
        "\n",
        "x_gpu"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 2., 4., 6., 8.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZKjydn8uwPG",
        "colab_type": "text"
      },
      "source": [
        "Okay, next see if you can create your own matrix multiplication kernel. Remember that there is a reduction operation in a matrix multiplication that needs to be run after the multiplication, so watch out for data races!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_PQ4U1hvPPj",
        "colab_type": "code",
        "outputId": "d020637f-9699-4f3a-db3e-f3d8989ee055",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "# Create your kernel here\n",
        "\n",
        "mod_mari = SourceModule(\"\"\"\n",
        "  __global__ void doublify(float *a)\n",
        "  {\n",
        "    int idx = threadIdx.x + threadIdx.y*4;\n",
        "    a[idx] *= 3;\n",
        "  }\n",
        "  \"\"\")\n",
        "\n",
        "func = mod_mari.get_function(\"doublify\")\n",
        "func(b_gpu, block=(4,4,1))\n",
        "\n",
        "b_doubled = np.empty_like(b)\n",
        "cuda.memcpy_dtoh(b_doubled, b_gpu)\n",
        "print(b_doubled)\n",
        "print(b)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-46.944664  -4.636413 -33.11819   -5.62018 ]\n",
            " [ 10.790088  19.951454  59.376472 -31.033104]\n",
            " [ 10.776745  40.776917 -20.283575 -12.632284]\n",
            " [-14.417896  31.72652  -14.61359   -8.637716]]\n",
            "[[-1.9560276  -0.19318388 -1.3799247  -0.23417418]\n",
            " [ 0.449587    0.8313106   2.4740198  -1.293046  ]\n",
            " [ 0.44903103  1.6990381  -0.845149   -0.5263452 ]\n",
            " [-0.6007457   1.3219384  -0.6088996  -0.35990486]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_xboPBqrnHa",
        "colab_type": "code",
        "outputId": "1fb86327-28e0-432c-f5ef-301aa1a979ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "mod2_mari = SourceModule(\"\"\"\n",
        "  __global__ void getInfo(int *a)\n",
        "  {\n",
        "    int idx = threadIdx.x;\n",
        "    a[idx] = threadIdx.x;\n",
        "\n",
        "  }\n",
        "  \"\"\")\n",
        "\n",
        "# Create an empty 1d array\n",
        "d = np.zeros((16), dtype=np.uint32)\n",
        "print(d)\n",
        "# Allocate GPU memory\n",
        "d_gpu = cuda.mem_alloc(d.nbytes)\n",
        "# clear the gpu memory\n",
        "cuda.memcpy_htod(d_gpu, d)\n",
        "\n",
        "# Get the function handle\n",
        "func2 = mod2_mari.get_function(\"getInfo\")\n",
        "# Create a lauch configuration with 4 threads and 1 block\n",
        "func2(d_gpu, block=(4,1,1))\n",
        "\n",
        "# Create array for output\n",
        "d_out = np.empty_like(d)\n",
        "# Copy the gpu memory to the host\n",
        "cuda.memcpy_dtoh(d_out, d_gpu)\n",
        "\n",
        "# Print data out\n",
        "print(d_out)\n",
        "\n",
        "# Free up the memory\n",
        "d_gpu.free()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 1 2 3 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COZzo3UzsxRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mod3_mari = SourceModule(\"\"\"\n",
        "  __global__ void getInfo(int *a)\n",
        "  {\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    a[idx] = idx;\n",
        "\n",
        "  }\n",
        "  \"\"\")\n",
        "\n",
        "\n",
        "# Allocate GPU memory\n",
        "d_gpu = cuda.mem_alloc(d.nbytes)\n",
        "# clear the gpu memory\n",
        "cuda.memcpy_htod(d_gpu, d)\n",
        "\n",
        "# Get the function handle\n",
        "func3 = mod3_mari.get_function(\"getInfo\")\n",
        "# Create a lauch configuration with 4 threads and 4 blocks\n",
        "func3(d_gpu, block=(4,1,1), grid=(4,1,1))\n",
        "\n",
        "# Copy the gpu memory to the host\n",
        "cuda.memcpy_dtoh(d_out, d_gpu)\n",
        "\n",
        "# Present data as a matrix\n",
        "print(d_out)\n",
        "\n",
        "# Free gpu memory\n",
        "d_gpu.free()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gE5XWcLh35v1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mod3_mari = SourceModule(\"\"\"\n",
        "  __global__ void getInfo(int *a)\n",
        "  {\n",
        "    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "    a[idx] = idx;\n",
        "\n",
        "  }\n",
        "  \"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ek5moRU3Bf31",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "27814333-1e3f-4715-c71d-42c12a177070"
      },
      "source": [
        "b = np.zeros((4,4,4), dtype=np.uint32)\n",
        "print(b)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0 0 0 0]\n",
            "  [0 0 0 0]\n",
            "  [0 0 0 0]\n",
            "  [0 0 0 0]]\n",
            "\n",
            " [[0 0 0 0]\n",
            "  [0 0 0 0]\n",
            "  [0 0 0 0]\n",
            "  [0 0 0 0]]\n",
            "\n",
            " [[0 0 0 0]\n",
            "  [0 0 0 0]\n",
            "  [0 0 0 0]\n",
            "  [0 0 0 0]]\n",
            "\n",
            " [[0 0 0 0]\n",
            "  [0 0 0 0]\n",
            "  [0 0 0 0]\n",
            "  [0 0 0 0]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "204LHvxpvV0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create your \n",
        "calling code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL1a_Eyivbpf",
        "colab_type": "text"
      },
      "source": [
        "Well done (or cmomiserations)!\n",
        "\n",
        "You have completed the introduction."
      ]
    }
  ]
}